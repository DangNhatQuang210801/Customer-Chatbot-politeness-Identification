ğŸŒ± What I built
This project is a full pipeline to test whether polite chatbot replies make people feel more satisfied and more willing to follow instructions.

Step 1 â€“ Stimuli
We start with real customer prompts and two chatbot replies (Chatbot A vs Chatbot B). Brands are hidden. Replies are paired so each prompt has a â€œless politeâ€ and a â€œmore politeâ€ version.

Step 2 â€“ Tags
Each reply is coded with politeness strategies (apology, empathy, hedges, refusal, next-step, etc.) using a frozen rulebook. This turns messy text into clean, machine-readable labels.

Step 3 â€“ Survey
We build survey blocks where participants see the A/B replies and rate them on politeness (PI), satisfaction (CSAT), clarity (HL), and compliance (CMP). Neutral filler items and attention checks keep responses honest.

Step 4 â€“ Data
Survey responses are cleaned and merged with metadata and tag labels. This produces tidy tables where each row is one participantâ€™s score for one reply.

Step 5 â€“ Checks
We run quality checks: reliability of survey items (Cronbachâ€™s Î±), attention-pass rates, and manipulation checks (whether B really scores higher on politeness tags than A).

Step 6 â€“ Analysis
We test A vs B differences with paired t-tests and mixed-effects models. This shows if polite replies truly improve satisfaction, helpfulness, or compliance across all prompts.

Step 7 â€“ Results & Reporting
We generate figures (bar charts with CIs, tag-effect plots), a minimal dashboard, and assemble notes into the final paper/report and presentation.

STIMULI & TASKS
Prompts.csv + Replies.csv / Replies-masked.csv â†’ Tasks-json.ipynb â†’ Replies\_Tasks.csv  \[pair\_id, Prompt\_Id, Reply\_Id, condition(A/B), bot\_mask, reply\_text]
Tag Book v1.docx + Tags.txt â†’ Rules\_book.ipynb â†’ tagbook\_v1\_1.json / tagbook\_v1\_1.md  \[frozen rules]
Replies\_Tasks.csv + tagbook\_v1\_1.json â†’ Rules\_book.ipynb â†’ reply\_tags.csv / Logical Tags Prediction.json  \[deterministic tags; no ML]

SURVEY BUILD
Replies\_Tasks.csv (+ optional reply\_tags.csv) â†’ survey\_form\_builder.ipynb â†’ survey\_form\_text\_AB.txt / survey\_form\_text\_BA.txt  \[20 items/reply; AB/BA]
survey\_form\_text\_\* + Replies\_Tasks.csv â†’ survey/make\_block\_map.ipynb â†’ surveyblock\_map.csv  \[maps item â†’ construct; marks reverse; attention\_flag + attention\_key]

SURVEY DATA â†’ TIDY SCORES
Google Forms CSV â€œChatbot Rating â€“ Anwers bot masked.csvâ€ + surveyblock\_map.csv â†’ src/preprocess.py â†’ answers\_long.csv / answers\_wide.csv  \[clean, reverse-code, PI/CSAT/HL/CMP means per personÃ—reply]
answers\_long.csv + Replies\_Tasks.csv + reply\_tags.csv â†’ src/preprocess.py â†’ analysis\_merged.csv  \[scores + metadata + tags]
(legacy) processed\_scores.xlsx / predictions.csv â†’ src/preprocess.py â†’ analysis\_merged.csv  \[aligned or deprecated]

QUALITY & MANIPULATION CHECKS
analysis\_merged.csv â†’ src/analysis.py â†’ reliability\_report.md / qc\_tables.csv  \[Cronbachâ€™s Î±; item stats; attention pass]
Replies\_Tasks.csv + reply\_tags.csv â†’ src/analysis.py â†’ mc\_checks.csv  \[A vs B tag-based Politeness Index per pair\_id/Prompt\_Id]
(alt) analysis\_merged.csv â†’ src/analysis.py â†’ mc\_checks.csv  \[same result via merged table]

MAIN ANALYSES
answers\_long.csv â†’ src/analysis.py â†’ ab\_welch\_per\_prompt.csv / ab\_summary\_overall.csv  \[paired tests by default; Cohenâ€™s d; 95% CIs]
analysis\_merged.csv â†’ src/analysis.py â†’ lme\_results.txt / model\_table.csv  \[mixed-effects: outcome \~ condition + (1|participant\_id) + (1|Prompt\_Id) Â± tag covariates]

FIGURES & REPORTING
ab\_summary\_overall.csv + mc\_checks.csv â†’ src/analysis.py â†’ figures/*.png  \[bars with CIs for PI/CSAT/HL/CMP; tag-effect plot]
analysis\_merged.csv â†’ dashboard.py â†’ Streamlit app  \[prompt â†’ replies â†’ tags/scores]
tagbook\_v1\_1.md + survey\_form\_text\_* + reliability\_report.md + ab\_summary\_overall.csv â†’ docs â†’ paper\_notes.md / appendix/

HOUSEKEEPING
Encoding: read/write CSV as UTF-8-SIG; if Windows-encoded, ingest via latin-1 then re-save UTF-8-SIG.
Masking: keep bot\_mask as A/B throughout; never reveal true brands.
Balance: enforce AB/BA quotas; track and correct in preprocessing.
Provenance: one script â†’ one output; deterministic seeds; tagbook v1.1 frozen.
