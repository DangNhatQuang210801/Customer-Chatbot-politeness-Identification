{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40a49a81-d7c4-4bff-9f61-855d865cbe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct counts: {'PI': 7000, 'HL': 2100, 'CMP': 2100, 'CSAT': 2100}\n",
      "construct counts: {'PI': 7000, 'HL': 2100, 'CMP': 2100, 'CSAT': 2100}\n",
      "metadata QA OK\n",
      "saved: processed_scores.csv columns: ['Prompt_Id', 'condition', 'CMP', 'CSAT', 'HL', 'PI', 'reply_id', 'bot_mask', 'reply_text']\n",
      "saved: answers_long.csv rows: 13300\n",
      "construct counts: {'PI': 7000, 'HL': 2100, 'CMP': 2100, 'CSAT': 2100}\n",
      "unique Prompt_Id: 5 unique conditions: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preprocess chatbot survey -> prompt x condition x scores\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SURVEY_FILE = Path(\"Answers_masked.csv\")\n",
    "BLOCKMAP_FILE = Path(\"surveyblock_map.csv\")\n",
    "TASKS_FILE = Path(\"tasks_data.json\")\n",
    "OUT_WIDE = Path(\"processed_scores.csv\")\n",
    "OUT_LONG = Path(\"answers_long.csv\")\n",
    "OUT_XLSX = Path(\"processed_scores.xlsx\")\n",
    "\n",
    "def detect_sep(first_line: str) -> str:\n",
    "    # pick ; if there are more semicolons than commas\n",
    "    return \";\" if first_line.count(\";\") > first_line.count(\",\") else \",\"\n",
    "\n",
    "def robust_read_csv(path: Path) -> pd.DataFrame:\n",
    "    # try common encodings, guess separator from first line\n",
    "    raw = path.read_bytes()\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
    "        try:\n",
    "            txt = raw.decode(enc, errors=\"strict\")\n",
    "            sep = detect_sep(txt.splitlines()[0] if txt else \",\")\n",
    "            return pd.read_csv(io.StringIO(txt), sep=sep)\n",
    "        except Exception:\n",
    "            continue\n",
    "    # last fallback\n",
    "    txt = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "    sep = detect_sep(txt.splitlines()[0] if txt else \",\")\n",
    "    return pd.read_csv(io.StringIO(txt), sep=sep)\n",
    "\n",
    "def norm(s):\n",
    "    # trim, remove non-breaking spaces, collapse internal spaces\n",
    "    s = str(s).replace(\"\\xa0\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "    \n",
    "def normalize_text(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Standardize smart quotes/whitespace in selected text cols.\"\"\"\n",
    "    repl = {\n",
    "        \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201C\": '\"', \"\\u201D\": '\"',\n",
    "        \"\\u00A0\": \" \", \"\\u200B\": \"\"\n",
    "    }\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            s = df[c].astype(str)\n",
    "            for k,v in repl.items(): s = s.str.replace(k, v, regex=False)\n",
    "            df[c] = s.str.strip()\n",
    "    return df\n",
    "    \n",
    "# load survey answers\n",
    "survey = robust_read_csv(SURVEY_FILE)\n",
    "survey.columns = [norm(c) for c in survey.columns]\n",
    "\n",
    "# add a simple participant id if missing\n",
    "if \"participant_id\" not in survey.columns:\n",
    "    survey.insert(0, \"participant_id\", [f\"R{i+1:03d}\" for i in range(len(survey))])\n",
    "\n",
    "# drop trailing language question if present\n",
    "if survey.columns[-1].lower().startswith(\"is your first language\"):\n",
    "    survey = survey.iloc[:, :-1]\n",
    "\n",
    "# load blockmap\n",
    "bm = robust_read_csv(BLOCKMAP_FILE)\n",
    "\n",
    "# pick the question text column name\n",
    "if \"question\" in bm.columns:\n",
    "    bm = bm.rename(columns={\"question\": \"question_text\"})\n",
    "elif \"column\" in bm.columns:\n",
    "    bm = bm.rename(columns={\"column\": \"question_text\"})\n",
    "else:\n",
    "    raise ValueError(\"surveyblock_map must have 'question' or 'column' for the question text\")\n",
    "\n",
    "# normalize key text columns if they exist\n",
    "for c in [\"question_text\", \"construct\", \"Prompt_Id\"]:\n",
    "    if c in bm.columns:\n",
    "        bm[c] = bm[c].map(norm)\n",
    "\n",
    "# make sure flag columns exist and are numeric\n",
    "for c in [\"reverse\", \"is_filler\", \"is_attention\"]:\n",
    "    if c not in bm.columns:\n",
    "        bm[c] = 0\n",
    "    bm[c] = pd.to_numeric(bm[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# infer condition from Prompt_Id if blockmap has no condition column\n",
    "if \"condition\" in bm.columns:\n",
    "    bm[\"condition\"] = bm[\"condition\"].astype(str).str.upper().str.strip()\n",
    "else:\n",
    "    bm[\"condition\"] = bm[\"Prompt_Id\"].astype(str).str.extract(r\"([AB])$\", expand=False)\n",
    "\n",
    "# reshape answers to long format\n",
    "ans_long = survey.melt(id_vars=[\"participant_id\"], var_name=\"question_text\", value_name=\"response\")\n",
    "ans_long[\"question_text\"] = ans_long[\"question_text\"].map(norm)\n",
    "ans_long[\"response\"] = pd.to_numeric(ans_long[\"response\"], errors=\"coerce\")\n",
    "\n",
    "# make a soft-normalized text key on both sides\n",
    "def soft_norm(s):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)     # remove punctuation\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip() # collapse spaces\n",
    "    return s\n",
    "\n",
    "# blockmap normalize\n",
    "bm[\"question_text\"] = bm[\"question_text\"].map(norm)\n",
    "bm[\"q_norm\"] = bm[\"question_text\"].map(soft_norm)\n",
    "\n",
    "# answers normalize\n",
    "ans_long[\"q_norm\"] = ans_long[\"question_text\"].map(soft_norm)\n",
    "\n",
    "# primary join by soft-normalized text\n",
    "df = ans_long.merge(bm, on=\"q_norm\", how=\"left\", suffixes=(\"\", \"_bm\"))\n",
    "\n",
    "# if some rows still missing construct, try a light keyword rule using the question text\n",
    "missing = df[\"construct\"].isna()\n",
    "if missing.any():\n",
    "    qt = df.loc[missing, \"question_text\"].str.lower()\n",
    "\n",
    "    # simple keyword hints\n",
    "    is_csat = qt.str.contains(\"satisf\")\n",
    "    is_hl   = qt.str.contains(\"next step\") | qt.str.contains(\"clear\")\n",
    "    is_cmp  = qt.str.contains(\"would follow\") | qt.str.contains(\"would do\")\n",
    "    is_pi   = qt.str.contains(\"polite\") | qt.str.contains(\"courteous\") | qt.str.contains(\"tone\") | qt.str.contains(\"please\") | qt.str.contains(\"could\")\n",
    "\n",
    "    # fill construct from keywords when available\n",
    "    df.loc[missing & is_csat, \"construct\"] = \"CSAT\"\n",
    "    df.loc[missing & is_hl,   \"construct\"] = \"HL\"\n",
    "    df.loc[missing & is_cmp,  \"construct\"] = \"CMP\"\n",
    "    df.loc[missing & is_pi,   \"construct\"] = \"PI\"\n",
    "\n",
    "    # also bring over Prompt_Id and condition by matching the A#/B# id\n",
    "    df[\"col_id\"] = df[\"question_text\"].str.extract(r\"\\b([AB]\\d{1,2})\\b\")\n",
    "    bm2 = bm.copy()\n",
    "    bm2[\"col_id\"] = bm2[\"question_text\"].str.extract(r\"\\b([AB]\\d{1,2})\\b\")\n",
    "    bm2 = bm2.dropna(subset=[\"col_id\"])\n",
    "    # merge only the id-based keys for rows still missing Prompt_Id/condition\n",
    "    id_merge = df.loc[missing, [\"q_norm\",\"col_id\"]].merge(\n",
    "        bm2[[\"col_id\",\"Prompt_Id\",\"condition\"]],\n",
    "        on=\"col_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df.loc[missing, \"Prompt_Id\"] = df.loc[missing, \"Prompt_Id\"].where(df.loc[missing, \"Prompt_Id\"].notna(), id_merge[\"Prompt_Id\"].values)\n",
    "    if \"condition\" not in df.columns:\n",
    "        df[\"condition\"] = np.nan\n",
    "    df.loc[missing, \"condition\"] = df.loc[missing, \"condition\"].where(df.loc[missing, \"condition\"].notna(), id_merge[\"condition\"].values)\n",
    "\n",
    "# standardize prompt id and condition\n",
    "m = df[\"Prompt_Id\"].astype(str).str.upper().str.extract(r\"^(P\\d{2,3})[_-]?([AB])?$\")\n",
    "df[\"Prompt_Id\"] = m[0]\n",
    "cond = df.get(\"condition\").astype(str).str.upper().str.strip()\n",
    "cond = np.where(cond.isin([\"A\",\"B\"]), cond, m[1])\n",
    "df[\"condition\"] = cond\n",
    "\n",
    "# keep only labeled rows now\n",
    "df = df[df[\"construct\"].notna()].copy()\n",
    "\n",
    "# drop filler items\n",
    "if \"is_filler\" in df.columns:\n",
    "    df = df[df[\"is_filler\"] != 1]\n",
    "\n",
    "# drop participants who fail attention check\n",
    "if (df[\"construct\"].astype(str).str.upper() == \"ATT_CHECK\").any():\n",
    "    att = (\n",
    "        df[df[\"construct\"].astype(str).str.upper() == \"ATT_CHECK\"]\n",
    "        .groupby(\"participant_id\")[\"response\"]\n",
    "        .apply(list).reset_index(name=\"att\")\n",
    "    )\n",
    "    def passed(xs):\n",
    "        xs = [v for v in xs if pd.notna(v)]\n",
    "        return int(len(xs) > 0 and all(v == 5 for v in xs))\n",
    "    att[\"pass\"] = att[\"att\"].apply(passed)\n",
    "    df = df.merge(att[[\"participant_id\", \"pass\"]], on=\"participant_id\", how=\"left\")\n",
    "    df = df[df[\"pass\"] == 1].drop(columns=[\"pass\"])\n",
    "\n",
    "# map construct names to canonical set\n",
    "canon = {\n",
    "    \"PI\": \"PI\", \"POLITENESS\": \"PI\",\n",
    "    \"CSAT\": \"CSAT\", \"SATISFACTION\": \"CSAT\", \"SAT\": \"CSAT\",\n",
    "    \"HL\": \"HL\", \"HELPFULNESS\": \"HL\", \"CLARITY\": \"HL\",\n",
    "    \"CMP\": \"CMP\", \"COMPLIANCE\": \"CMP\"\n",
    "}\n",
    "df[\"construct\"] = df[\"construct\"].astype(str).str.upper().map(lambda x: canon.get(x, x))\n",
    "\n",
    "# keep target constructs\n",
    "targets = {\"PI\",\"CSAT\",\"HL\",\"CMP\"}\n",
    "df = df[df[\"construct\"].isin(targets)].copy()\n",
    "assert (df[\"construct\"] == \"CSAT\").any(), \"CSAT missing after merge/filter\"\n",
    "\n",
    "print(\"construct counts:\", df[\"construct\"].value_counts().to_dict())\n",
    "\n",
    "# fallback merge by A/B item id for rows that did not match by text\n",
    "# safer approach: build a unique mapping and left-join, then fill missing fields\n",
    "\n",
    "# make column id in df for all rows\n",
    "df[\"col_id\"] = df[\"question_text\"].str.extract(r\"\\b([AB]\\d{1,2})\\b\")\n",
    "\n",
    "# make a unique mapping table from blockmap\n",
    "bm2 = bm.copy()\n",
    "bm2[\"col_id\"] = bm2[\"question_text\"].str.extract(r\"\\b([AB]\\d{1,2})\\b\")\n",
    "bm2 = bm2[[\"col_id\", \"construct\", \"reverse\", \"Prompt_Id\", \"condition\"]]\n",
    "bm2 = bm2.dropna(subset=[\"col_id\"]).drop_duplicates(subset=[\"col_id\"], keep=\"first\")\n",
    "\n",
    "# join mapping onto df (suffix _byid)\n",
    "df = df.merge(bm2, on=\"col_id\", how=\"left\", suffixes=(\"\", \"_byid\"))\n",
    "\n",
    "# fill only where original is missing\n",
    "for c in [\"construct\", \"reverse\", \"Prompt_Id\", \"condition\"]:\n",
    "    filler = f\"{c}_byid\"\n",
    "    if filler in df.columns:\n",
    "        df[c] = df[c].where(df[c].notna(), df[filler])\n",
    "\n",
    "# cleanup helper columns\n",
    "drop_cols = [c for c in [\"col_id\", \"construct_byid\", \"reverse_byid\", \"Prompt_Id_byid\", \"condition_byid\"] if c in df.columns]\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# canonical construct labels + keep targets\n",
    "canon = {\n",
    "    \"PI\":\"PI\",\"POLITENESS\":\"PI\",\n",
    "    \"CSAT\":\"CSAT\",\"SATISFACTION\":\"CSAT\",\"SAT\":\"CSAT\",\n",
    "    \"HL\":\"HL\",\"HELPFULNESS\":\"HL\",\"CLARITY\":\"HL\",\n",
    "    \"CMP\":\"CMP\",\"COMPLIANCE\":\"CMP\"\n",
    "}\n",
    "targets = {\"PI\",\"CSAT\",\"HL\",\"CMP\"}\n",
    "df[\"construct\"] = df[\"construct\"].astype(str).str.upper().map(lambda x: canon.get(x, x))\n",
    "df = df[df[\"construct\"].isin(targets)].copy()\n",
    "assert (df[\"construct\"] == \"CSAT\").any(), \"CSAT missing after merge/filter\"\n",
    "\n",
    "# reverse-score ONCE using the final 'reverse' flags\n",
    "df[\"score\"] = df[\"response\"]\n",
    "df.loc[df[\"reverse\"] == 1, \"score\"] = 6 - df.loc[df[\"reverse\"] == 1, \"score\"]\n",
    "\n",
    "# drop filler\n",
    "if \"is_filler\" in df.columns:\n",
    "    df = df[df[\"is_filler\"] != 1]\n",
    "\n",
    "# attention-check (use raw responses; attention items must be 5)\n",
    "if (df[\"construct\"] == \"ATT_CHECK\").any():\n",
    "    att = (df[df[\"construct\"] == \"ATT_CHECK\"]\n",
    "           .groupby(\"participant_id\")[\"response\"].apply(list).reset_index(name=\"att\"))\n",
    "    def passed(xs):\n",
    "        xs = [v for v in xs if pd.notna(v)]\n",
    "        return int(len(xs) > 0 and all(v == 5 for v in xs))\n",
    "    att[\"pass\"] = att[\"att\"].apply(passed)\n",
    "    df = df.merge(att[[\"participant_id\",\"pass\"]], on=\"participant_id\", how=\"left\")\n",
    "    df = df[df[\"pass\"] == 1].drop(columns=[\"pass\"])\n",
    "\n",
    "# standardize prompt id and condition\n",
    "m = df[\"Prompt_Id\"].astype(str).str.upper().str.extract(r\"^(P\\d{2,3})[_-]?([AB])?$\")\n",
    "df[\"Prompt_Id\"] = m[0]\n",
    "cond = df.get(\"condition\").astype(str).str.upper().str.strip()\n",
    "cond = np.where(cond.isin([\"A\", \"B\"]), cond, m[1])\n",
    "df[\"condition\"] = cond\n",
    "\n",
    "# keep only labeled rows\n",
    "df = df[df[\"construct\"].notna()].copy()\n",
    "\n",
    "# reverse score items marked as reverse (Likert 1..5 -> 6-x)\n",
    "df.loc[df[\"reverse\"] == 1, \"response\"] = 6 - df.loc[df[\"reverse\"] == 1, \"response\"]\n",
    "\n",
    "# drop filler items\n",
    "df = df[df[\"is_filler\"] != 1]\n",
    "\n",
    "# drop participants who fail attention check (all attention items must be 5)\n",
    "if (df[\"construct\"].str.upper() == \"ATT_CHECK\").any():\n",
    "    att = (\n",
    "        df[df[\"construct\"].str.upper() == \"ATT_CHECK\"]\n",
    "        .groupby(\"participant_id\")[\"response\"]\n",
    "        .apply(list)\n",
    "        .reset_index(name=\"att\")\n",
    "    )\n",
    "    def passed(xs):\n",
    "        xs = [v for v in xs if pd.notna(v)]\n",
    "        return int(len(xs) > 0 and all(v == 5 for v in xs))\n",
    "    att[\"pass\"] = att[\"att\"].apply(passed)\n",
    "    df = df.merge(att[[\"participant_id\", \"pass\"]], on=\"participant_id\", how=\"left\")\n",
    "    df = df[df[\"pass\"] == 1].drop(columns=[\"pass\"])\n",
    "\n",
    "# keep target constructs\n",
    "targets = {\"PI\", \"CSAT\", \"HL\", \"CMP\"}\n",
    "df[\"construct\"] = df[\"construct\"].str.upper()\n",
    "df = df[df[\"construct\"].isin(targets)].copy()\n",
    "\n",
    "# quick count by construct to confirm CSAT is present\n",
    "print(\"construct counts:\", df[\"construct\"].value_counts().to_dict())\n",
    "\n",
    "# aggregate to prompt x condition x construct\n",
    "scored = (\n",
    "    df.groupby([\"Prompt_Id\", \"condition\", \"construct\"])[\"response\"]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "wide = (\n",
    "    scored.pivot(index=[\"Prompt_Id\", \"condition\"], columns=\"construct\", values=\"response\")\n",
    "          .reset_index()\n",
    ")\n",
    "wide.columns.name = None\n",
    "for c in targets:\n",
    "    if c in wide.columns:\n",
    "        wide[c] = wide[c].round(3)\n",
    "\n",
    "# try to merge task metadata if available\n",
    "try:\n",
    "    tasks = json.loads(Path(TASKS_FILE).read_text(encoding=\"utf-8\"))\n",
    "    tdf = pd.json_normalize(tasks)\n",
    "    if \"meta.prompt_id\" not in tdf.columns and \"meta.Prompt_Id\" in tdf.columns:\n",
    "        tdf = tdf.rename(columns={\"meta.Prompt_Id\": \"meta.prompt_id\",\n",
    "                                  \"meta.Reply_Id\": \"meta.reply_id\"})\n",
    "    tdf[\"Prompt_Id\"] = tdf[\"meta.prompt_id\"].astype(str).str.upper()\n",
    "    tdf[\"condition\"] = tdf[\"meta.condition\"].astype(str).str.upper()\n",
    "    tdf[\"reply_id\"] = tdf[\"meta.reply_id\"].astype(str)\n",
    "    tdf[\"bot_mask\"] = tdf.get(\"data.bot_mask\", np.nan)\n",
    "    tdf[\"reply_text\"] = tdf.get(\"data.reply_text\", np.nan)\n",
    "    tdf = tdf[[\"Prompt_Id\", \"condition\", \"reply_id\", \"bot_mask\", \"reply_text\"]].drop_duplicates()\n",
    "    out = wide.merge(tdf, on=[\"Prompt_Id\", \"condition\"], how=\"left\")\n",
    "except Exception as e:\n",
    "    print(\"tasks_data.json merge skipped:\", e)\n",
    "    out = wide.copy()\n",
    "\n",
    "# metadata loader: normalize Replies-masked.csv to expected columns\n",
    "import pandas as pd\n",
    "\n",
    "def load_metadata(path=\"Replies-masked.csv\"):\n",
    "    \"\"\"\n",
    "    Normalize reply metadata to:\n",
    "      Prompt_Id (P##), condition ('A'/'B'), reply_id, bot_mask, reply_text.\n",
    "    Robust to encoding/sep, header variants, and missing 'condition'.\n",
    "    \"\"\"\n",
    "    df = robust_read_csv(Path(path))  # your existing robust reader\n",
    "\n",
    "    # normalize header names\n",
    "    def clean_name(s: str) -> str:\n",
    "        s = str(s).strip().lower()\n",
    "        s = re.sub(r\"\\s+\", \"_\", s)\n",
    "        s = s.replace(\"‐\", \"-\")  # hyphen variants\n",
    "        return s\n",
    "\n",
    "    df.columns = [clean_name(c) for c in df.columns]\n",
    "\n",
    "    # candidates by heuristic\n",
    "    col_prompt = next((c for c in df.columns if c in {\"prompt_id\",\"prompt\",\"promptid\",\"p_id\"} or \"prompt\" in c), None)\n",
    "    col_cond   = next((c for c in df.columns if c in {\"condition\",\"cond\",\"ab\"} or \"cond\" in c), None)\n",
    "    col_rid    = next((c for c in df.columns if c in {\"reply_id\",\"replyid\",\"rid\"} or (\"reply\" in c and \"id\" in c)), None)\n",
    "    col_rtext  = next((c for c in df.columns if c in {\"reply_text\",\"reply\",\"response_text\"} or (\"reply\" in c and \"text\" in c)), None)\n",
    "    col_bot    = next((c for c in df.columns if c in {\"bot_mask\",\"bot\",\"botname\"} or (\"bot\" in c and \"mask\" in c)), None)\n",
    "\n",
    "    # rename to canonical\n",
    "    ren = {}\n",
    "    if col_prompt: ren[col_prompt] = \"Prompt_Id\"\n",
    "    if col_cond:   ren[col_cond]   = \"condition\"\n",
    "    if col_rid:    ren[col_rid]    = \"reply_id\"\n",
    "    if col_rtext:  ren[col_rtext]  = \"reply_text\"\n",
    "    if col_bot:    ren[col_bot]    = \"bot_mask\"\n",
    "    if ren: df = df.rename(columns=ren)\n",
    "\n",
    "    # type/casing cleanup\n",
    "    for c in [\"Prompt_Id\",\"condition\",\"reply_id\"]:\n",
    "        if c not in df: df[c] = pd.NA\n",
    "        df[c] = df[c].astype(str).str.strip().str.upper()\n",
    "    for c in [\"bot_mask\",\"reply_text\"]:\n",
    "        if c not in df: df[c] = pd.NA\n",
    "        df[c] = df[c].astype(str)\n",
    "\n",
    "    # infer condition if missing/empty: try Prompt_Id like \"P03_A\" or \"P03A\"\n",
    "    if \"condition\" not in df or df[\"condition\"].replace({\"\": pd.NA}).isna().all():\n",
    "        pid = df.get(\"Prompt_Id\", pd.Series([], dtype=str)).astype(str).str.upper()\n",
    "        cond = pid.str.extract(r\"(?:^|[_-])(A|B)(?:$|[_-])\", expand=False)\n",
    "        # fallback: last A/B in the string if pattern fail\n",
    "        cond2 = pid.str.findall(r\"[AB]\").apply(lambda xs: xs[-1] if len(xs) else pd.NA)\n",
    "        df[\"condition\"] = cond.fillna(cond2)\n",
    "\n",
    "    # as a second fallback, infer from any column that only contains A/B\n",
    "    if df[\"condition\"].isna().any():\n",
    "        ab_cols = [c for c in df.columns if set(df[c].dropna().astype(str).str.upper().unique()) <= {\"A\",\"B\"}]\n",
    "        if ab_cols:\n",
    "            df.loc[df[\"condition\"].isna(), \"condition\"] = df.loc[df[\"condition\"].isna(), ab_cols[0]].astype(str).str.upper()\n",
    "\n",
    "    # final standardization\n",
    "    df[\"Prompt_Id\"] = df[\"Prompt_Id\"].str.upper().str.extract(r\"(P\\d{2,3})\", expand=False)\n",
    "    df[\"condition\"] = df[\"condition\"].str.upper().where(df[\"condition\"].isin([\"A\",\"B\"]))\n",
    "\n",
    "    # normalize smart quotes/nbsp\n",
    "    df = normalize_text(df, [\"Prompt_Id\",\"condition\",\"reply_text\",\"bot_mask\"])\n",
    "\n",
    "    keep = [\"Prompt_Id\",\"condition\",\"reply_id\",\"bot_mask\",\"reply_text\"]\n",
    "    return df[keep].drop_duplicates()\n",
    "  \n",
    "# --- metadata QA: require BOTH A and B per prompt, then check identical texts ---\n",
    "meta = load_metadata(\"Replies-masked.csv\")\n",
    "meta = normalize_text(meta, [\"Prompt_Id\",\"condition\",\"reply_text\",\"bot_mask\"])\n",
    "meta[\"Prompt_Id\"] = meta[\"Prompt_Id\"].astype(str).str.upper().str.extract(r\"(P\\d{2,3})\", expand=False)\n",
    "meta[\"condition\"] = meta[\"condition\"].astype(str).str.upper().str.strip()\n",
    "meta = meta[meta[\"condition\"].isin([\"A\",\"B\"])]\n",
    "\n",
    "cond_sets = meta.groupby(\"Prompt_Id\")[\"condition\"].apply(lambda s: set(s.dropna()))\n",
    "lacking = cond_sets[~cond_sets.apply(lambda s: {\"A\",\"B\"}.issubset(s))].index.tolist()\n",
    "assert not lacking, f\"prompts without both A and B: {lacking}\"\n",
    "\n",
    "pairs = (meta.pivot_table(index=\"Prompt_Id\", columns=\"condition\",\n",
    "                          values=\"reply_text\", aggfunc=\"first\")\n",
    "           .reindex(columns=[\"A\",\"B\"])\n",
    "           .dropna(subset=[\"A\",\"B\"]))\n",
    "ident_prompts = pairs.index[pairs[\"A\"] == pairs[\"B\"]].tolist()\n",
    "assert not ident_prompts, f\"identical A/B replies for: {ident_prompts}\"\n",
    "print(\"metadata QA OK\")\n",
    "\n",
    "# merge metadata into scores at the end\n",
    "out = wide.merge(meta, on=[\"Prompt_Id\",\"condition\"], how=\"left\")\n",
    "\n",
    "# save outputs\n",
    "out.to_csv(OUT_WIDE, index=False, encoding=\"utf-8-sig\")\n",
    "df.to_csv(OUT_LONG, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "with pd.ExcelWriter(OUT_XLSX) as xw:\n",
    "    out.to_excel(xw, sheet_name=\"scores\", index=False)\n",
    "    df.head(5000).to_excel(xw, sheet_name=\"answers_long_preview\", index=False)\n",
    "\n",
    "print(\"saved:\", OUT_WIDE, \"columns:\", list(out.columns))\n",
    "print(\"saved:\", OUT_LONG, \"rows:\", len(df))\n",
    "print(\"construct counts:\", df[\"construct\"].astype(str).str.upper().value_counts().to_dict())\n",
    "print(\"unique Prompt_Id:\", df[\"Prompt_Id\"].nunique(), \"unique conditions:\", df[\"condition\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cb1a360-f389-46d3-845f-017092cb8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ab_welch_per_prompt.csv and ab_summary_overall.csv\n",
      "  metric  k_prompts  mean_diff      t      p  ci_low  ci_high    d_z\n",
      "0     PI          5     -0.192 -3.728  0.020  -0.335   -0.049 -1.667\n",
      "1   CSAT          3     -0.092 -0.907  0.460  -0.527    0.343 -0.523\n",
      "2     HL          3      0.095  0.483  0.677  -0.751    0.941  0.279\n",
      "3    CMP          3     -0.411 -1.141  0.372  -1.962    1.140 -0.659\n"
     ]
    }
   ],
   "source": [
    "# A vs B (independent) analysis from answers_long.csv\n",
    "# Saves per-prompt Welch tests and an across-prompt summary with 3dp rounding.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "IN_LONG = Path(\"answers_long.csv\")\n",
    "OUT_PER_PROMPT = Path(\"ab_welch_per_prompt.csv\")\n",
    "OUT_SUMMARY = Path(\"ab_summary_overall.csv\")\n",
    "\n",
    "# load and filter\n",
    "df = pd.read_csv(IN_LONG, encoding=\"utf-8-sig\")\n",
    "metrics = [\"PI\", \"CSAT\", \"HL\", \"CMP\"]\n",
    "df = df[df[\"construct\"].isin(metrics)].copy()\n",
    "df[\"condition\"] = df[\"condition\"].astype(str).str.upper().str.strip()\n",
    "df = df[df[\"condition\"].isin([\"A\", \"B\"])].copy()\n",
    "df[\"response\"] = pd.to_numeric(df[\"response\"], errors=\"coerce\")\n",
    "\n",
    "# collapse to participant means per Prompt × Condition × Metric\n",
    "dfp = (df.groupby([\"participant_id\", \"Prompt_Id\", \"condition\", \"construct\"])[\"response\"]\n",
    "         .mean()\n",
    "         .reset_index()\n",
    "         .rename(columns={\"construct\": \"metric\"}))\n",
    "\n",
    "def welch_test(a, b):\n",
    "    \"\"\"\n",
    "    Welch's t on independent samples a vs b.\n",
    "    Returns stats with diff = mean(b) - mean(a), and t aligned to that diff.\n",
    "    \"\"\"\n",
    "    a = pd.Series(a, dtype=\"float64\").dropna()\n",
    "    b = pd.Series(b, dtype=\"float64\").dropna()\n",
    "    n1, n2 = len(a), len(b)\n",
    "    if n1 < 2 or n2 < 2:\n",
    "        return dict(nA=n1, nB=n2, meanA=a.mean(), meanB=b.mean(), diff=b.mean() - a.mean(),\n",
    "                    t=np.nan, df=np.nan, p=np.nan, d=np.nan, g=np.nan,\n",
    "                    ci_low=np.nan, ci_high=np.nan)\n",
    "\n",
    "    # order matters: ttest_ind(b, a) so t matches diff = B - A\n",
    "    t, p = stats.ttest_ind(b, a, equal_var=False, nan_policy=\"omit\")\n",
    "\n",
    "    s1, s2 = a.var(ddof=1), b.var(ddof=1)\n",
    "    se = np.sqrt(s1/n1 + s2/n2)\n",
    "\n",
    "    df_w = (s1/n1 + s2/n2) ** 2 / ((s1**2) / (n1**2 * (n1 - 1)) + (s2**2) / (n2**2 * (n2 - 1)))\n",
    "    tcrit = stats.t.ppf(0.975, df=df_w) if np.isfinite(df_w) else np.nan\n",
    "\n",
    "    sp = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    d = (b.mean() - a.mean()) / sp if np.isfinite(sp) and sp > 0 else np.nan\n",
    "    J = 1 - 3 / (4 * (n1 + n2) - 9) if (n1 + n2) > 3 else np.nan\n",
    "    g = d * J if np.isfinite(d) and np.isfinite(J) else np.nan\n",
    "\n",
    "    diff = b.mean() - a.mean()\n",
    "    ci_low = diff - tcrit * se if np.isfinite(tcrit) else np.nan\n",
    "    ci_high = diff + tcrit * se if np.isfinite(tcrit) else np.nan\n",
    "\n",
    "    return dict(nA=n1, nB=n2, meanA=a.mean(), meanB=b.mean(), diff=diff,\n",
    "                t=t, df=df_w, p=p, d=d, g=g, ci_low=ci_low, ci_high=ci_high)\n",
    "\n",
    "# per-prompt Welch tests on participant means\n",
    "rows = []\n",
    "for m in metrics:\n",
    "    sub = dfp[dfp[\"metric\"] == m]\n",
    "    for pid, grp in sub.groupby(\"Prompt_Id\"):\n",
    "        a = grp.loc[grp[\"condition\"] == \"A\", \"response\"]\n",
    "        b = grp.loc[grp[\"condition\"] == \"B\", \"response\"]\n",
    "        res = welch_test(a, b)\n",
    "        res.update(metric=m, Prompt_Id=pid)\n",
    "        rows.append(res)\n",
    "\n",
    "per_prompt = pd.DataFrame(rows)\n",
    "per_prompt = per_prompt[[\"metric\",\"Prompt_Id\",\"nA\",\"nB\",\"meanA\",\"meanB\",\"diff\",\"t\",\"df\",\"p\",\"d\",\"g\",\"ci_low\",\"ci_high\"]]\n",
    "per_prompt = per_prompt.sort_values([\"metric\",\"Prompt_Id\"]).reset_index(drop=True)\n",
    "\n",
    "# round for output\n",
    "pp_cols = [\"meanA\",\"meanB\",\"diff\",\"t\",\"df\",\"p\",\"d\",\"g\",\"ci_low\",\"ci_high\"]\n",
    "per_prompt[pp_cols] = per_prompt[pp_cols].round(3)\n",
    "per_prompt.to_csv(OUT_PER_PROMPT, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# across-prompt summary: 1-sample t on per-prompt diffs\n",
    "sum_rows = []\n",
    "for m in metrics:\n",
    "    dvals = per_prompt.loc[per_prompt[\"metric\"] == m, \"diff\"].astype(float).dropna()\n",
    "    if dvals.empty:\n",
    "        sum_rows.append(dict(metric=m, k_prompts=0, mean_diff=np.nan, t=np.nan, p=np.nan,\n",
    "                             ci_low=np.nan, ci_high=np.nan, d_z=np.nan))\n",
    "        continue\n",
    "    t1, p1 = stats.ttest_1samp(dvals, 0.0, nan_policy=\"omit\")\n",
    "    mean_diff = dvals.mean()\n",
    "    sd = dvals.std(ddof=1) if len(dvals) > 1 else np.nan\n",
    "    se = sd/np.sqrt(len(dvals)) if np.isfinite(sd) and sd > 0 else np.nan\n",
    "    tcrit = stats.t.ppf(0.975, df=len(dvals)-1) if len(dvals) > 1 else np.nan\n",
    "    ci_low = mean_diff - tcrit*se if np.isfinite(tcrit) else np.nan\n",
    "    ci_high = mean_diff + tcrit*se if np.isfinite(tcrit) else np.nan\n",
    "    dz = mean_diff / sd if np.isfinite(sd) and sd > 0 else np.nan\n",
    "    sum_rows.append(dict(metric=m, k_prompts=len(dvals), mean_diff=round(mean_diff,3),\n",
    "                         t=t1, p=p1, ci_low=ci_low, ci_high=ci_high, d_z=dz))\n",
    "\n",
    "summary = pd.DataFrame(sum_rows)\n",
    "summary[[\"t\",\"p\",\"ci_low\",\"ci_high\",\"d_z\"]] = summary[[\"t\",\"p\",\"ci_low\",\"ci_high\",\"d_z\"]].round(3)\n",
    "summary.to_csv(OUT_SUMMARY, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"saved\", OUT_PER_PROMPT, \"and\", OUT_SUMMARY)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea40ee-34cd-43e4-b095-fe7401f35eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
